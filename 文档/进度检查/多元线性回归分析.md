# 多元线性回归分析

作出以多个自变量估计因变量的多元线性回归方程。

# 思路

- ## 特征放缩(归一化)

  标准化数据特征的范围，保证不同的特征在一个数量级，这样梯度下降收敛快，加快梯度的收敛

- ## 损失函数

  - **通过最小化损失函数求解和评估模型**

  - **回归问题**

    - L2：均方误差损失函数

    $$
    \Large
    	MSE=\frac{\sum_{i=1}^{n}(y_i-y_i^{p})^{2}}{n}
    $$

    ​	L2损失通过平方计算放大了估计值和真实值的距离，因此对偏离观测值的输出给予很大的惩罚。此外，L2损失是平滑函数，在求解其优化问题时有利于误差梯度的计算

    

    - L1：平均绝对误差损失函数

    $$
    \Large
    MAE=\frac{\sum_{i=1}^{n}|y_i-y_i^{p}|}{n}
    $$

    平均绝对误差（MAE）是另一种常用的回归损失函数，它是目标值与预测值之差绝对值的和，表示了预测值的平均误差幅度，对偏离真实值的输出不敏感，因此在观测中存在异常值时有利于保持模型稳定，且不需要考虑误差的方向，范围是0到∞

  - **问题转换为求解L2或L1的最小值**

  如果我们计算能力无限大，直接穷举就完了，但是这不是高效的办法，这时候就到了下面要说的梯度下降法

- ## 梯度下降法
  在微积分里面，对多元函数的参数求∂偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。比如函数f（n，m）, 分别对n，m求偏导数，求得的梯度向量就是(∂f / ∂n, ∂f / ∂m)T，简称grad f（n，m）或者▽ f（n，m）
  $$
  \Large
  gradf(n,m)=
  	\nabla
  		\left\{
  			\frac{\partial f}{\partial s}
  			+
  			\frac{\partial f}{\partial m}
  		\right\}
  $$

  - **作用**

    1. 函数增大啊方向
    2. 走向增大的方向，应该走多大步幅

  - **常见的梯度下降法**

    1. 批量梯度下降法BDG
    2. 随机梯度下降法SDG

  - **为什么最终是选择批量梯度下降法**

    1. 数据量相对较小，由每次迭代对所有样本计算带来的训练过程很慢的缺点影响不大
    2. 每一次迭代都对所有样本进行计算，此时利用矩阵进行操作，实现了并行
    3. 由全数据集确定的方向能更好地代表样本总体，从而更准确地朝向极值所在的方向，并且当目标函数为凸函数时，BGD一定能够得到全局最优
    4. SGD较于BGD其准确度较低
    5. SGD不是在全部训练数据上的损失函数，而是在每轮迭代中，随机优化某一条训练数据上的损失函，由于单个样本并不能代表全体样本的趋势，SGD可能会收敛到局部最优，

  - **问题**

    对于求上述损失函数的极小值，我们反方向走即可，加个负号，但是关于梯度告诉我们该走的步幅有个问题，如果过大，参数就直接飞出去了，就很难在找到最小值，如果太小，则很有可能卡在局部极小值的地方。所以一般使用一个系数来调节步幅，也就是下面要说的学习速率`learningRate`。

  - 

- ## 学习率

在梯度下降的过程中更新权重时的超参数，学习率越低，损失函数的变化速度就越慢，容易过拟合。虽然使用低学习率可以确保我们不会错过任何局部极小值，但也意味着我们将花费更长的时间来进行收敛，特别是在被困在局部最优点的时候。而学习率过高容易发生梯度爆炸，loss振动幅度较大，模型难以收敛。下图是不同学习率的loss变化，因此，选择一个合适的学习率是十分重要的



# 过程

![image-20210405115505195](C:\Users\ZeyFra\AppData\Roaming\Typora\typora-user-images\image-20210405115505195.png)

正负号对不对是基于一些经验判断的，比如你觉得这个自变量应该是对因变量有个正的影响，那是否是真实的确是有正的影响，这个是需要看数据分析的。比如如果你觉得很明显的违反了常识性的正负，那表示自变量可能存在一定的共线性，可以进行一下共线性分析看是否如此

**解决方案**

- 移除出共线性的自变量

先做下相关分析，如果发现某两个自变量X（解释变量）的相关系数值大于0.7，则移除掉一个自变量（解释变量），然后再做回归分析。但此种办法有一个小问题，即有的时候根本就不希望把某个自变量从模型中剔除，如果有此类情况，可考虑使用逐步回归让软件自动剔除，同时更优的办法可能是使用岭回归进行分析。

- 逐步回归法

让软件自动进行自变量的选择剔除，逐步回归会将共线性的自变量自动剔除出去。此种解决办法有个问题是，可能算法会剔除掉本不想剔除的自变量，如果有此类情况产生，此时最好是使用岭回归进行分析。

- 岭回归

第1和第2种解决办法在实际研究中使用较多，但问题在于，如果实际研究中并不想剔除掉某些自变量，某些自变量很重要，不能剔除。此时可能只有岭回归最为适合了。岭回归是当前解决共线性问题最有效的解释办法，但是岭回归的分析相对较为复杂，后面会提供具体例子，当然也可以参考SPSSAU官网岭回归说明。

- 利用因子分析合并变量

共线性问题的解释办法是，理论上可以考虑使用因子分析（或者主成分分析），利用数学变换，将数据降维提取成几个成分，即把信息进行浓缩，最后以浓缩后的信息作为自变量（解释变量）进入 模型进行分析。此种解释办法在理论上可行，而且有效。但实际研究中会出现一个问题，即本身研究的X1,X2,X3等，进行了因子分析(或主成分）后，变成成分1，成分2类似这样的了，意义完全与实际研究情况不符合，这导致整个研究的思路也会变换，因而此种办法适用于探索性研究时使用，而不适合实际验证性研究。

- 改变解释变量的形式

对于横截面数据采用相对数变量，对于时间序列数据采用增量型变量